{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Malignant Tumor Diagnosis With Pytorch\n",
    "#### Objective\n",
    "Exercise in initiating a logistic regression using Pytorch and comparing two common optimser functions when it comes to loss reduction:\n",
    "\n",
    "* Activation function: Sigmoid.\n",
    "* Optimiser functions: Stochastic gradient decent & Resilient backpropagation.\n",
    "\n",
    "\n",
    "#### Dataset\n",
    "`https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29`\n",
    "* Diagnosis (1 = malignant, 0 = benign).\n",
    "* 30 measurements.\n",
    "\n",
    "#### Finding\n",
    "Rprop apears to be a far more effective method but needs more exploration as to why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "xy = np.loadtxt('data.csv', delimiter=',', skiprows=1, dtype=np.float32)\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 30])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(torch.from_numpy(xy[:,1:31]))\n",
    "y_data = Variable(torch.from_numpy(xy[:,[0]]))\n",
    "\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        ''' 4 layers'''\n",
    "        self.l1 = torch.nn.Linear(30, 20)\n",
    "        self.l2 = torch.nn.Linear(20, 15)\n",
    "        self.l3 = torch.nn.Linear(15, 5)\n",
    "        self.l4 = torch.nn.Linear(5, 1)\n",
    "\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        out3 = self.sigmoid(self.l3(out2))\n",
    "        y_pred = self.sigmoid(self.l4(out3))\n",
    "        return y_pred\n",
    "        \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Error: Binary cross entropy loss; Optimiser: Stochastic gradient decent'''\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7002820372581482\n",
      "1 0.6998130679130554\n",
      "2 0.6993486285209656\n",
      "3 0.6988902688026428\n",
      "4 0.6984373331069946\n",
      "5 0.697989284992218\n",
      "6 0.6975465416908264\n",
      "7 0.697110116481781\n",
      "8 0.6966780424118042\n",
      "9 0.6962502598762512\n",
      "10 0.6958273649215698\n",
      "11 0.6954104900360107\n",
      "12 0.6949977874755859\n",
      "13 0.6945894360542297\n",
      "14 0.6941863298416138\n",
      "15 0.6937887668609619\n",
      "16 0.693393886089325\n",
      "17 0.6930049061775208\n",
      "18 0.692620038986206\n",
      "19 0.6922404766082764\n",
      "20 0.691863477230072\n",
      "21 0.6914914846420288\n",
      "22 0.6911247968673706\n",
      "23 0.6907603144645691\n",
      "24 0.6904012560844421\n",
      "25 0.6900460124015808\n",
      "26 0.6896964311599731\n",
      "27 0.6893491744995117\n",
      "28 0.6890063881874084\n",
      "29 0.6886669993400574\n",
      "30 0.6883322596549988\n",
      "31 0.6880006194114685\n",
      "32 0.6876736879348755\n",
      "33 0.6873494386672974\n",
      "34 0.687030017375946\n",
      "35 0.686713695526123\n",
      "36 0.6864011883735657\n",
      "37 0.6860923767089844\n",
      "38 0.6857863068580627\n",
      "39 0.6854845285415649\n",
      "40 0.6851862072944641\n",
      "41 0.6848912835121155\n",
      "42 0.6845988631248474\n",
      "43 0.6843109130859375\n",
      "44 0.6840260624885559\n",
      "45 0.683743953704834\n",
      "46 0.6834661960601807\n",
      "47 0.683190643787384\n",
      "48 0.6829184293746948\n",
      "49 0.6826496124267578\n",
      "50 0.6823839545249939\n",
      "51 0.6821216940879822\n",
      "52 0.6818614602088928\n",
      "53 0.6816055178642273\n",
      "54 0.681351363658905\n",
      "55 0.6811006665229797\n",
      "56 0.6808525919914246\n",
      "57 0.6806080937385559\n",
      "58 0.6803661584854126\n",
      "59 0.6801270842552185\n",
      "60 0.6798906326293945\n",
      "61 0.6796569228172302\n",
      "62 0.6794257164001465\n",
      "63 0.6791977882385254\n",
      "64 0.6789725422859192\n",
      "65 0.6787493228912354\n",
      "66 0.6785291433334351\n",
      "67 0.6783111095428467\n",
      "68 0.6780961751937866\n",
      "69 0.6778835654258728\n",
      "70 0.6776731014251709\n",
      "71 0.6774653792381287\n",
      "72 0.6772602796554565\n",
      "73 0.6770574450492859\n",
      "74 0.6768574714660645\n",
      "75 0.6766588687896729\n",
      "76 0.6764630079269409\n",
      "77 0.6762694716453552\n",
      "78 0.6760785579681396\n",
      "79 0.6758893132209778\n",
      "80 0.6757020950317383\n",
      "81 0.675517737865448\n",
      "82 0.6753352284431458\n",
      "83 0.6751551032066345\n",
      "84 0.67497718334198\n",
      "85 0.6748006939888\n",
      "86 0.6746277213096619\n",
      "87 0.674455463886261\n",
      "88 0.6742850542068481\n",
      "89 0.6741180419921875\n",
      "90 0.6739519238471985\n",
      "91 0.6737874150276184\n",
      "92 0.6736259460449219\n",
      "93 0.6734659075737\n",
      "94 0.673307478427887\n",
      "95 0.6731511354446411\n",
      "96 0.6729965209960938\n",
      "97 0.6728441119194031\n",
      "98 0.6726927757263184\n",
      "99 0.6725439429283142\n",
      "100 0.672396719455719\n",
      "101 0.6722506284713745\n",
      "102 0.6721062064170837\n",
      "103 0.6719648838043213\n",
      "104 0.6718238592147827\n",
      "105 0.6716841459274292\n",
      "106 0.671547532081604\n",
      "107 0.6714107990264893\n",
      "108 0.6712775230407715\n",
      "109 0.671144962310791\n",
      "110 0.671013593673706\n",
      "111 0.6708839535713196\n",
      "112 0.6707565188407898\n",
      "113 0.6706299781799316\n",
      "114 0.6705045104026794\n",
      "115 0.670381486415863\n",
      "116 0.6702592372894287\n",
      "117 0.670139491558075\n",
      "118 0.6700195670127869\n",
      "119 0.6699020862579346\n",
      "120 0.6697856187820435\n",
      "121 0.669670820236206\n",
      "122 0.6695573329925537\n",
      "123 0.6694440245628357\n",
      "124 0.6693341135978699\n",
      "125 0.6692230105400085\n",
      "126 0.6691153049468994\n",
      "127 0.6690083742141724\n",
      "128 0.6689025163650513\n",
      "129 0.6687972545623779\n",
      "130 0.6686946749687195\n",
      "131 0.6685916185379028\n",
      "132 0.6684908270835876\n",
      "133 0.6683909893035889\n",
      "134 0.6682922840118408\n",
      "135 0.6681949496269226\n",
      "136 0.6680979132652283\n",
      "137 0.6680034399032593\n",
      "138 0.6679084897041321\n",
      "139 0.6678156852722168\n",
      "140 0.6677238941192627\n",
      "141 0.6676326394081116\n",
      "142 0.6675427556037903\n",
      "143 0.667453408241272\n",
      "144 0.6673669219017029\n",
      "145 0.6672786474227905\n",
      "146 0.6671937704086304\n",
      "147 0.6671093106269836\n",
      "148 0.6670257449150085\n",
      "149 0.6669424176216125\n",
      "150 0.6668602228164673\n",
      "151 0.666779100894928\n",
      "152 0.6666995882987976\n",
      "153 0.666621208190918\n",
      "154 0.6665419340133667\n",
      "155 0.6664651036262512\n",
      "156 0.6663886904716492\n",
      "157 0.6663130521774292\n",
      "158 0.6662392020225525\n",
      "159 0.6661656498908997\n",
      "160 0.666092574596405\n",
      "161 0.666020929813385\n",
      "162 0.6659500002861023\n",
      "163 0.6658797264099121\n",
      "164 0.6658099889755249\n",
      "165 0.6657418012619019\n",
      "166 0.6656737923622131\n",
      "167 0.6656060218811035\n",
      "168 0.6655400991439819\n",
      "169 0.6654744148254395\n",
      "170 0.6654096841812134\n",
      "171 0.6653461456298828\n",
      "172 0.6652820706367493\n",
      "173 0.6652193069458008\n",
      "174 0.6651579737663269\n",
      "175 0.6650970578193665\n",
      "176 0.6650367379188538\n",
      "177 0.6649770140647888\n",
      "178 0.6649178862571716\n",
      "179 0.66485995054245\n",
      "180 0.6648017764091492\n",
      "181 0.66474449634552\n",
      "182 0.6646881699562073\n",
      "183 0.6646327376365662\n",
      "184 0.6645775437355042\n",
      "185 0.6645240783691406\n",
      "186 0.6644692420959473\n",
      "187 0.6644170880317688\n",
      "188 0.6643633246421814\n",
      "189 0.664311945438385\n",
      "190 0.6642613410949707\n",
      "191 0.6642102003097534\n",
      "192 0.6641596555709839\n",
      "193 0.6641101837158203\n",
      "194 0.664061427116394\n",
      "195 0.6640127301216125\n",
      "196 0.6639640927314758\n",
      "197 0.663917601108551\n",
      "198 0.6638700366020203\n",
      "199 0.6638244986534119\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    '''forward pass'''\n",
    "    y_pred = model(x_data) # initial prediction    \n",
    "    loss = criterion(y_pred, y_data) # calculate loss\n",
    "    print(epoch, loss.item())\n",
    "    \n",
    "    '''backward pass'''\n",
    "    optimiser.zero_grad() # initialise gradients\n",
    "    loss.backward() # perform backward pass\n",
    "    optimiser.step() # update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''New optimiser: Resilient backpropagation'''\n",
    "optimiser = torch.optim.Rprop(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.66377854347229\n",
      "1 0.661592423915863\n",
      "2 0.6598459482192993\n",
      "3 0.6587503552436829\n",
      "4 0.6577660441398621\n",
      "5 0.6568313241004944\n",
      "6 0.6540791988372803\n",
      "7 0.6523354649543762\n",
      "8 0.649625301361084\n",
      "9 0.6426085829734802\n",
      "10 0.630620002746582\n",
      "11 0.6214485168457031\n",
      "12 0.6079702377319336\n",
      "13 0.5876423120498657\n",
      "14 0.5584074854850769\n",
      "15 0.5276049375534058\n",
      "16 0.4959976375102997\n",
      "17 0.4622417092323303\n",
      "18 0.43823009729385376\n",
      "19 0.4113892912864685\n",
      "20 0.3926478624343872\n",
      "21 0.36922407150268555\n",
      "22 0.3555181324481964\n",
      "23 0.34290191531181335\n",
      "24 0.32979363203048706\n",
      "25 0.3218981921672821\n",
      "26 0.31706079840660095\n",
      "27 0.3175273835659027\n",
      "28 0.31574103236198425\n",
      "29 0.31319087743759155\n",
      "30 0.31174978613853455\n",
      "31 0.3096717298030853\n",
      "32 0.3060515522956848\n",
      "33 0.3006279468536377\n",
      "34 0.2933281362056732\n",
      "35 0.2841844856739044\n",
      "36 0.2730571925640106\n",
      "37 0.2586275041103363\n",
      "38 0.24052362143993378\n",
      "39 0.22056402266025543\n",
      "40 0.19890253245830536\n",
      "41 0.18035085499286652\n",
      "42 0.1749805361032486\n",
      "43 0.16377124190330505\n",
      "44 0.15099690854549408\n",
      "45 0.1396896094083786\n",
      "46 0.14139722287654877\n",
      "47 0.1282292604446411\n",
      "48 0.12484301626682281\n",
      "49 0.11959763616323471\n",
      "50 0.11298709362745285\n",
      "51 0.10766297578811646\n",
      "52 0.10665830224752426\n",
      "53 0.10453750938177109\n",
      "54 0.10086698830127716\n",
      "55 0.09710139781236649\n",
      "56 0.09287908673286438\n",
      "57 0.08928138762712479\n",
      "58 0.08611641824245453\n",
      "59 0.08312904834747314\n",
      "60 0.08050871640443802\n",
      "61 0.07784352451562881\n",
      "62 0.07590073347091675\n",
      "63 0.07397349178791046\n",
      "64 0.0721566379070282\n",
      "65 0.07004180550575256\n",
      "66 0.06782691925764084\n",
      "67 0.0656338632106781\n",
      "68 0.0634206011891365\n",
      "69 0.0613861009478569\n",
      "70 0.059441015124320984\n",
      "71 0.05870271846652031\n",
      "72 0.05700370669364929\n",
      "73 0.056136514991521835\n",
      "74 0.05487437546253204\n",
      "75 0.05381716415286064\n",
      "76 0.05269743874669075\n",
      "77 0.05153321847319603\n",
      "78 0.05035034939646721\n",
      "79 0.04901047796010971\n",
      "80 0.0472334586083889\n",
      "81 0.04670320823788643\n",
      "82 0.04515013471245766\n",
      "83 0.043524593114852905\n",
      "84 0.04321657493710518\n",
      "85 0.04230935499072075\n",
      "86 0.0419624038040638\n",
      "87 0.04126540571451187\n",
      "88 0.04055890440940857\n",
      "89 0.03988972678780556\n",
      "90 0.03940984606742859\n",
      "91 0.03892765939235687\n",
      "92 0.03842524439096451\n",
      "93 0.03787952661514282\n",
      "94 0.037332598119974136\n",
      "95 0.03675830364227295\n",
      "96 0.03649909794330597\n",
      "97 0.03594130277633667\n",
      "98 0.0356091670691967\n",
      "99 0.035098299384117126\n",
      "100 0.03463945910334587\n",
      "101 0.03408585116267204\n",
      "102 0.03348641097545624\n",
      "103 0.032838184386491776\n",
      "104 0.03212525695562363\n",
      "105 0.031351976096630096\n",
      "106 0.030541665852069855\n",
      "107 0.02970261126756668\n",
      "108 0.02915355935692787\n",
      "109 0.028558282181620598\n",
      "110 0.02800772897899151\n",
      "111 0.02752530761063099\n",
      "112 0.0271145012229681\n",
      "113 0.026667527854442596\n",
      "114 0.026192063465714455\n",
      "115 0.025690531358122826\n",
      "116 0.025158552452921867\n",
      "117 0.02460239827632904\n",
      "118 0.02403760887682438\n",
      "119 0.02349698357284069\n",
      "120 0.023020561784505844\n",
      "121 0.022617710754275322\n",
      "122 0.02242361381649971\n",
      "123 0.0220709927380085\n",
      "124 0.021797720342874527\n",
      "125 0.021468866616487503\n",
      "126 0.02111630141735077\n",
      "127 0.02070966362953186\n",
      "128 0.02026071958243847\n",
      "129 0.019781189039349556\n",
      "130 0.01978025957942009\n",
      "131 0.019118396565318108\n",
      "132 0.01883736439049244\n",
      "133 0.01844685524702072\n",
      "134 0.018111830577254295\n",
      "135 0.017791669815778732\n",
      "136 0.01758071780204773\n",
      "137 0.017312845215201378\n",
      "138 0.01703791879117489\n",
      "139 0.016896463930606842\n",
      "140 0.016726043075323105\n",
      "141 0.016576239839196205\n",
      "142 0.016446983441710472\n",
      "143 0.016316866502165794\n",
      "144 0.01618964597582817\n",
      "145 0.016054747626185417\n",
      "146 0.01591341756284237\n",
      "147 0.015778429806232452\n",
      "148 0.015658479183912277\n",
      "149 0.015590015798807144\n",
      "150 0.015551649034023285\n",
      "151 0.01543681975454092\n",
      "152 0.015463242307305336\n",
      "153 0.015378979034721851\n",
      "154 0.015304758213460445\n",
      "155 0.01528493408113718\n",
      "156 0.015233522281050682\n",
      "157 0.01519845798611641\n",
      "158 0.015155415050685406\n",
      "159 0.015112736262381077\n",
      "160 0.01506687793880701\n",
      "161 0.015015645883977413\n",
      "162 0.014961261302232742\n",
      "163 0.014904011972248554\n",
      "164 0.014843117445707321\n",
      "165 0.014780495315790176\n",
      "166 0.014712092466652393\n",
      "167 0.014639465138316154\n",
      "168 0.014563015662133694\n",
      "169 0.014482231810688972\n",
      "170 0.014401936903595924\n",
      "171 0.01432124339044094\n",
      "172 0.01424673106521368\n",
      "173 0.01419754046946764\n",
      "174 0.01413933839648962\n",
      "175 0.014097936451435089\n",
      "176 0.014043588191270828\n",
      "177 0.013991233892738819\n",
      "178 0.01392578985542059\n",
      "179 0.01385053712874651\n",
      "180 0.013772466219961643\n",
      "181 0.01370429527014494\n",
      "182 0.013635552488267422\n",
      "183 0.013566353358328342\n",
      "184 0.013493911363184452\n",
      "185 0.013417050242424011\n",
      "186 0.013345333747565746\n",
      "187 0.01328999176621437\n",
      "188 0.013234507292509079\n",
      "189 0.013175617903470993\n",
      "190 0.013113110326230526\n",
      "191 0.013047651387751102\n",
      "192 0.01298474334180355\n",
      "193 0.01292459573596716\n",
      "194 0.012876761145889759\n",
      "195 0.01285552978515625\n",
      "196 0.0128159886226058\n",
      "197 0.012759115546941757\n",
      "198 0.012727382592856884\n",
      "199 0.012712893076241016\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    '''forward pass'''\n",
    "    y_pred = model(x_data) # initial prediction    \n",
    "    loss = criterion(y_pred, y_data) # calculate loss\n",
    "    print(epoch, loss.item())\n",
    "    \n",
    "    '''backward pass'''\n",
    "    optimiser.zero_grad() # initialise gradients\n",
    "    loss.backward() # perform backward pass\n",
    "    optimiser.step() # update the weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
